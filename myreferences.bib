% A %

% B %

% C %

% D %

% P $
% article -- journal article
% inProceedings -- conference proceedings
% misc -- news articles, medium.com, ...
% PhDThesis -- 
% MSCSThesis --


@article{gueta_how_2013,
	title = {How Travel Pattern Changes after Number Coding Scheme as a Travel Demand Management Measure was Implemented?},
	abstract = {Due to serious increase of vehicle ownership in Metro Manila, number coding scheme ({NCS}) was introduced as travel demand management ({TDM}) measure to control the use of vehicles especially during rush hours. The scheme works by employing the last digit of vehicle’s plate number to restrict them from running the streets. This study seeks to investigate and explore the possible effects of {NCS} on commuter’s travel pattern, both the use of private and public transport for daily trips. Interview survey was conducted in early 2011 to gather the pertinent data used in the analysis. From the empirical investigation, the results reveal that during number coding scheme day, when their car is banned from plying the streets, commuter would still use their car in expense of extending their total activity time – meaning car use is not necessarily reduced but shifted to other times of the day.},
	author = {Gueta, Grace Padayhag and Gueta, Lounell Bahoy},
	year = {2013},
}



@article{roy_why_2020,
	title = {Why is traffic congestion getting worse? {A} decomposition of the contributors to growing congestion in {San} {Francisco}-{Determining} the {Role} of {TNCs}},
	volume = {8},
	issn = {2213-624X},
	shorttitle = {Why is traffic congestion getting worse?},
	url = {https://www.sciencedirect.com/science/article/pii/S2213624X20301048},
	doi = {10.1016/j.cstp.2020.09.008},
	abstract = {Traffic congestion has worsened noticeably in San Francisco and other major cities over the past few years. This change could reasonably be explained by strong economic growth or other standard factors such as road and transit network changes. However, the worsening congestion also corresponds to the emergence of Transportation Network Companies (TNCs), such as Uber and Lyft, raising the question of whether the two trends may be related. Our research decomposes the contributors to increased congestion in San Francisco between 2010 and 2016, considering contributions from five incremental effects: road and transit network changes, population growth, employment growth, TNC volumes, and the effect of TNC pick-ups and drop-offs. We do so through a series of controlled travel demand model runs, supplemented with observed TNC data collected from the Application Programming Interfaces (APIs) of Uber and Lyft. Our results show that road and transit network changes over this period have only a small effect on congestion, population and employment growth each contribute about a quarter of the congestion increase, and that TNCs are the biggest contributor to growing congestion over this period, contributing about half of the increase in vehicle hours of delay, and adding to worsening travel time reliability. This research contradicts several studies that suggest TNCs may reduce congestion, and adds evidence in support of other recent empirical analyses showing that their net effect is to increase congestion. It is more data rich and spatially detailed than past studies, providing a better understanding of where and when TNCs add to congestion. This research gives transportation planners a better understanding of the causes of growing congestion, allowing them to more effectively craft strategies to mitigate or adapt to it.},
	number = {4},
	urldate = {2024-03-08},
	journal = {Case Studies on Transport Policy},
	author = {Roy, Sneha and Cooper, Drew and Mucci, Alex and Sana, Bhargava and Chen, Mei and Castiglione, Joe and Erhardt, Gregory D.},
	month = dec,
	year = {2020},
}


@article{ning_vehicular_2019,
	title = {Vehicular {Fog} {Computing}: {Enabling} {Real}-{Time} {Traffic} {Management} for {Smart} {Cities}},
	volume = {26},
	issn = {1558-0687},
	shorttitle = {Vehicular {Fog} {Computing}},
	url = {https://ieeexplore.ieee.org/document/8641431},
	doi = {10.1109/MWC.2019.1700441},
	abstract = {Fog computing extends the facility of cloud computing from the center to edge networks. Although fog computing has the advantages of location awareness and low latency, the rising requirements of ubiquitous connectivity and ultra-low latency challenge real-time traffic management for smart cities. As an integration of fog computing and vehicular networks, vehicular fog computing (VFC) is promising to achieve real-time and location-aware network responses. Since the concept and use case of VFC are in the initial phase, this article first constructs a three-layer VFC model to enable distributed traffic management in order to minimize the response time of citywide events collected and reported by vehicles. Furthermore, the VFC-enabled offloading scheme is formulated as an optimization problem by leveraging moving and parked vehicles as fog nodes. A real-world taxi-trajectory-based performance analysis validates our model. Finally, some research challenges and open issues toward VFC-enabled traffic management are summarized and highlighted.},
	number = {1},
	urldate = {2024-03-08},
	journal = {IEEE Wireless Communications},
	author = {Ning, Zhaolong and Huang, Jun and Wang, Xiaojie},
	month = feb,
	year = {2019},
}


@article{han_attention_2024,
	title = {An {Attention} {Reinforcement} {Learning}–{Based} {Strategy} for {Large}-{Scale} {Adaptive} {Traffic} {Signal} {Control} {System}},
	volume = {150},
	issn = {2473-2907, 2473-2893},
	url = {https://ascelibrary.org/doi/10.1061/JTEPBS.TEENG-8261},
	doi = {10.1061/JTEPBS.TEENG-8261},
	abstract = {This paper proposes a reinforcement learning (RL)-based traffic control strategy integrated with attention mechanism for large-scale adaptive traffic signal control (ATSC) system. The proposed attention RL integrates attention mechanism into a multiagent RL model, namely multiagent proximal policy optimization (MAPPO), so as to enable more effective, scalable, and stable learning in complex ATSC environments. In the attention RL, decentralized policies are trained using a centrally computed critic that shares an attention model, while the attention model selects relevant intersections for each agent to estimate the global critic. This framework effectively reduces the computational complexity and stabilizes the training process, enhancing the ability of RL agents to control large-scale traffic networks. The proposed control strategy is tested in both a large synthetic traffic grid and a large real-world traffic network of Yangzhou city using the microscopic traffic simulation tool, SUMO. Experimental results demonstrate that the proposed approach learns stable and sustainable policies that achieve lower congestion level and faster recovery, which outperforms other state-of-art RL-based approaches, as well as a gap-based actuated controller. DOI: 10.1061/JTEPBS.TEENG-8261. © 2024 American Society of Civil Engineers.},
	language = {en},
	number = {3},
	urldate = {2024-02-19},
	journal = {Journal of Transportation Engineering, Part A: Systems},
	author = {Han, Gengyue and Liu, Xiaohan and Wang, Hao and Dong, Changyin and Han, Yu},
	month = mar,
	year = {2024},
	keywords = {notion},
	pages = {04024001},
	file = {Han et al. - 2024 - An Attention Reinforcement Learning–Based Strategy.pdf:C\:\\Users\\Jet\\Zotero\\storage\\MU45NUDZ\\Han et al. - 2024 - An Attention Reinforcement Learning–Based Strategy.pdf:application/pdf},
}


@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@article{WalravenErwin2016TfoA,
abstract = {Traffic congestion causes important problems such as delays, increased fuel consumption and additional pollution. In this paper we propose a new method to optimize traffic flow, based on reinforcement learning. We show that a traffic flow optimization problem can be formulated as a Markov Decision Process. We use Q-learning to learn policies dictating the maximum driving speed that is allowed on a highway, such that traffic congestion is reduced. An important difference between our work and existing approaches is that we take traffic predictions into account. A series of simulation experiments shows that the resulting policies significantly reduce traffic congestion under high traffic demand, and that inclusion of traffic predictions improves the quality of the resulting policies. Additionally, the policies are sufficiently robust to deal with inaccurate speed and density measurements.
•We model a traffic flow optimization problem as a reinforcement learning problem.•We show how speed limit policies can be obtained using Q-learning.•Neural networks improve the performance of our policy learning algorithm.•Resulting policies are able to significantly reduce traffic congestion.•Our method takes traffic predictions into account and controls proactively.},
author = {Walraven, Erwin and Spaan, Matthijs T.J. and Bakker, Bram},
copyright = {2016 Elsevier Ltd},
issn = {0952-1976},
journal = {Engineering applications of artificial intelligence},
publisher = {Elsevier Ltd},
title = {Traffic flow optimization: A reinforcement learning approach},
volume = {52},
year = {2016},
}

@article{CAI2009456,
title = {Adaptive traffic signal control using approximate dynamic programming},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {17},
number = {5},
pages = {456-474},
year = {2009},
note = {Artificial Intelligence in Transportation Analysis: Approaches, Methods, and Applications},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2009.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X09000321},
author = {Chen Cai and Chi Kwong Wong and Benjamin G. Heydecker},
keywords = {Traffic signal, Dynamic programming, Approximation, Adaptive, Reinforcement learning},
abstract = {This paper presents a study on an adaptive traffic signal controller for real-time operation. The controller aims for three operational objectives: dynamic allocation of green time, automatic adjustment to control parameters, and fast revision of signal plans. The control algorithm is built on approximate dynamic programming (ADP). This approach substantially reduces computational burden by using an approximation to the value function of the dynamic programming and reinforcement learning to update the approximation. We investigate temporal-difference learning and perturbation learning as specific learning techniques for the ADP approach. We find in computer simulation that the ADP controllers achieve substantial reduction in vehicle delays in comparison with optimised fixed-time plans. Our results show that substantial benefits can be gained by increasing the frequency at which the signal plans are revised, which can be achieved conveniently using the ADP approach.}
}


@misc{seanflemming_traffic_2019,
	title = {Traffic congestion cost the {US} economy nearly \$87 billion in 2018},
	url = {https://www.weforum.org/agenda/2019/03/traffic-congestion-cost-the-us-economy-nearly-87-billion-in-2018/},
	abstract = {The older the city, the more winding its road network is apt to be. And as such cities have grown, they’ve struggled with the volume of traffic on their roads. Could the latest tech help alleviate congestion?},
	language = {en},
        author = {Flemming, Sean},
        year = {2019},
	urldate = {2024-03-08},
	journal = {World Economic Forum},
	month = mar,
	year = {2019},
}


@misc{cnnphilippines_mmda_2018,
	title = {{MMDA} says traffic schemes ease {EDSA} congestion - {CNN} {Philippines}},
	url = {https://web.archive.org/web/20181018195602/https://www.cnnphilippines.com/news/2018/08/15/mmda-single-passenger-ban-edsa.html},
        author={{CNN Philippines}},
        author={Janine Peralta},
        year={2018},
	urldate = {2024-03-08},
}


@article{dulac-arnold_challenges_2021,
	title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
	volume = {110},
	copyright = {© The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature 2021.},
	issn = {08856125},
	shorttitle = {Challenges of real-world reinforcement learning},
	url = {https://www.proquest.com/docview/2566146084/abstract/576CDCD66922475FPQ/1},
	doi = {10.1007/s10994-021-05961-4},
	abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
	language = {English},
	number = {9},
	urldate = {2024-03-08},
	journal = {Machine Learning},
	author = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Cosmin, Paduraru and Sven, Gowal and Hester, Todd},
	month = sep,
	year = {2021},
}


@article{mousavi2017traffic,
  title={Traffic light control using deep policy-gradient and value-function-based reinforcement learning},
  author={Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  journal={IET Intelligent Transport Systems},
  volume={11},
  number={7},
  pages={417--423},
  year={2017},
  publisher={Wiley Online Library}
}

@article{wan2018value,
  title={Value-based deep reinforcement learning for adaptive isolated intersection signal control},
  author={Wan, Chia-Hao and Hwang, Ming-Chorng},
  journal={IET Intelligent Transport Systems},
  volume={12},
  number={9},
  pages={1005--1010},
  year={2018},
  publisher={Wiley Online Library}
}

@article{tang2020semi,
  title={Semi-supervised double duelling broad reinforcement learning in support of traffic service in smart cities},
  author={Tang, Jing and Wei, Xin and Zhao, Jialin and Gao, Yun},
  journal={IET Intelligent Transport Systems},
  volume={14},
  number={10},
  pages={1278--1285},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{gunarathna2021real,
  title={Real-time lane configuration with coordinated reinforcement learning},
  author={Gunarathna, Udesh and Xie, Hairuo and Tanin, Egemen and Karunasekara, Shanika and Borovica-Gajic, Renata},
  booktitle={Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14--18, 2020, Proceedings, Part IV},
  pages={291--307},
  year={2021},
  organization={Springer}
}

@inproceedings{wang_dynamic_2018,
  author={Wang, Chen and David, Bertrand and Chalon, René},
  booktitle={2014 International Conference on Advanced Logistics and Transport (ICALT)}, 
  title={Dynamic road lane management: A smart city application}, 
  year={2014},
  doi={10.1109/ICAdLT.2014.6864085}}


@article{wang_shortening_2022,
	title = {Shortening {Passengers}’ {Travel} {Time}: {A} {Dynamic} {Metro} {Train} {Scheduling} {Approach} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {Shortening {Passengers}’ {Travel} {Time}},
	url = {https://ieeexplore.ieee.org/document/9729432},
	doi = {10.1109/TKDE.2022.3153385},
	abstract = {Urban metros have become the foremost public transit to modern cities, carrying millions of daily rides. As travel efficiency matters to the work productivity of the city, shortening passengers’ travel time for metros is therefore a pressing need, which can bring substantial economic benefits. In this paper, we study a fine-grained, safe, and energy-efficient strategy to improve the efficiency of metro systems by dynamically scheduling dwell time for trains. However, developing such a strategy is very challenging because of three aspects: 1) The objective of optimizing the average travel time of passengers is complex, as it needs to properly balance passengers’ waiting time at platforms and journey time on trains, as well as considering long-term impacts on the whole metro system; 2) Capturing dynamic spatio-temporal (ST) correlations of incoming passengers for metro stations is difficult; and 3) For each train, the dwell time scheduling is affected by other trains on the same metro line, which is not easy to measure. To tackle these challenges, we propose a novel deep neural network, entitled AutoDwell. Specifically, AutoDwell optimizes the long-term rewards of dwell time settings in terms of passengers’ waiting time at platforms and journey time on trains by a reinforcement learning framework. Next, AutoDwell employs gated recurrent units and graph attention networks to extract the ST correlations of the passenger flows among metro stations. In addition, attention mechanisms are leveraged in AutoDwell for capturing the interactions between the trains on the same metro line. Extensive experiments on two real-world datasets collected from Beijing and Hangzhou, China, demonstrate the superior performance of AutoDwell over several baselines, capable of saving passengers’ overall travel time. In particular, the model can shorten the waiting time by at least 9\%, which can boost passengers’ experience significantly.},
	number = {5},
	urldate = {2024-03-08},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Zhaoyuan and Pan, Zheyi and Chen, Shun and Ji, Shenggong and Yi, Xiuwen and Zhang, Junbo and Wang, Jingyuan and Gong, Zhiguo and Li, Tianrui and Zheng, Yu},
	month = may,
	year = 2022,
}

@article{ying_actor-critic_2020,
	title = {An actor-critic deep reinforcement learning approach for metro train scheduling with rolling stock circulation under stochastic demand},
	volume = {140},
	issn = {0191-2615},
	url = {https://www.sciencedirect.com/science/article/pii/S0191261520303829},
	doi = {10.1016/j.trb.2020.08.005},
	abstract = {This paper presents a novel actor-critic deep reinforcement learning approach for metro train scheduling with circulation of limited rolling stock. The scheduling problem is modeled as a Markov decision process driven by stochastic passenger demand. As in most dynamic optimization problems, the complexity of the scheduling process grows exponentially with the amount of states, decisions, and uncertainties involved. This study aims to address this ‘curses of dimensionality’ issue by adopting an actor-critic deep reinforcement learning solution framework. The framework simplifies the evaluation and searching process for potential optimal solutions by parameterizing the original state and decision spaces with the use of artificial neural networks. A deep deterministic policy gradient algorithm is developed for training the artificial neural networks via simulated system transitions before the actor-critic agent can be applied for online schedule control. The proposed approach is tested with a real-world scenario configured with data collected from the Victoria Line of London Underground, UK. Experiment results illustrate the advantages of the proposed method over a range of established meta-heuristics in terms of computing time, system efficiency, and robustness under different stochastic environments. This study innovates urban transit operations with state-of-the-art computer science and dynamic optimization techniques.},
	urldate = {2024-03-08},
	journal = {Transportation Research Part B: Methodological},
	author = {Ying, Cheng-shuo and Chow, Andy H. F. and Chin, Kwai-Sang},
	month = oct,
	year = {2020}
}


@phdthesis{wirtz2004using,
  title={Using simulation to test traffic incident management strategies: Illustrating the benefits of pre-planning},
  author={Wirtz, John James},
  year={2004},
  school={Northwestern University}
}


@inproceedings{martin_train_1999,
	address = {Phoenix, Arizona, United States},
	title = {Train performance and simulation},
	volume = {2},
	isbn = {978-0-7803-5780-8},
	url = {http://portal.acm.org/citation.cfm?doid=324898.325068},
	doi = {10.1145/324898.325068},
	language = {en},
	urldate = {2024-03-12},
	booktitle = {Proceedings of the 31st conference on {Winter} simulation {Simulation}---a bridge to the future - {WSC} '99},
	publisher = {ACM Press},
	author = {Martin, Paul},
	year = {1999},
}

@inproceedings{greenshields1935study,
  title={A study of traffic capacity},
  author={Greenshields, Bruce D and Bibbins, J Rowland and Channing, WS and Miller, Harvey H},
  booktitle={Highway research board proceedings},
  volume={14},
  number={1},
  pages={448--477},
  year={1935},
  organization={Washington, DC}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@techreport{drew1968traffic,
  title={Traffic flow theory and control},
  author={Drew, Donald R},
  year={1968}
}

@article{prigogine1961boltzmann,
  title={A Boltzmann-like approach to the statistical theory of traffic flow},
  author={Prigogine, Ilya},
  journal={Theory of traffic flow},
  year={1961},
  publisher={Elsevier Pub. Co.}
}

@book{payne1971models,
  title={Models of Freeway Traffic and Control},
  author={Payne, H.J.},
  url={https://books.google.com.ph/books?id=1X4ZcgAACAAJ},
  year={1971},
  publisher={Simulation Councils, Incorporated}
}

@inproceedings{backfrieder2013traffsim,
  title={TraffSim--A Traffic Simulator for Investigating Benefits Ensuing from Intelligent Traffic Management},
  author={Backfrieder, Christian and Mecklenbr{\"a}uker, Christoph F and Ostermayer, Gerald},
  booktitle={2013 European Modelling Symposium},
  pages={451--456},
  year={2013},
  organization={IEEE}
}


@article{chu_multi-agent_2020,
	title = {Multi-{Agent} {Deep} {Reinforcement} {Learning} for {Large}-{Scale} {Traffic} {Signal} {Control}},
	volume = {21},
	issn = {1558-0016},
	url = {https://ieeexplore-ieee-org.dlsu.idm.oclc.org/document/8667868},
	doi = {10.1109/TITS.2019.2901791},
	abstract = {Reinforcement learning (RL) is a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, and deep neural networks further enhance its learning power. However, the centralized RL is infeasible for large-scale ATSC due to the extremely high dimension of the joint action space. The multi-agent RL (MARL) overcomes the scalability issue by distributing the global control to each local RL agent, but it introduces new challenges: now, the environment becomes partially observable from the viewpoint of each local agent due to limited communication among agents. Most existing studies in MARL focus on designing efficient communication and coordination among traditional Q-learning agents. This paper presents, for the first time, a fully scalable and decentralized MARL algorithm for the state-of-the-art deep RL agent, advantage actor critic (A2C), within the context of ATSC. In particular, two methods are proposed to stabilize the learning procedure, by improving the observability and reducing the learning difficulty of each local agent. The proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms, in both a large synthetic traffic grid and a large real-world traffic network of Monaco city, under simulated peak-hour traffic dynamics. The results demonstrate its optimality, robustness, and sample efficiency over the other state-of-the-art decentralized MARL algorithms.},
	number = {3},
	urldate = {2024-07-01},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Chu, Tianshu and Wang, Jie and Codecà, Lara and Li, Zhaojian},
	month = mar,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {actor-critic, Adaptive traffic signal control, Codecs, Convergence, cooperative, deep reinforcement learning, Heuristic algorithms, Mathematical model, multi-agent, multi-agent reinforcement learning, Neural networks, reinforcement learning, Reinforcement learning, Scalability},
	pages = {1086--1095},
	file = {IEEE Xplore Abstract Record:/Users/mark.cortejo/Zotero/storage/8W652VBS/8667868.html:text/html;IEEE Xplore Full Text PDF:/Users/mark.cortejo/Zotero/storage/UHPBUDFL/Chu et al. - 2020 - Multi-Agent Deep Reinforcement Learning for Large-.pdf:application/pdf},
}

@inproceedings{nishi_traffic_2018,
	title = {Traffic {Signal} {Control} {Based} on {Reinforcement} {Learning} with {Graph} {Convolutional} {Neural} {Nets}},
	url = {https://ieeexplore-ieee-org.dlsu.idm.oclc.org/document/8569301},
	doi = {10.1109/ITSC.2018.8569301},
	abstract = {Traffic signal control can mitigate traffic congestion and reduce travel time. A model-free reinforcement learning (RL) approach is a powerful framework for learning a responsive traffic control policy for short-term traffic demand changes without prior environmental knowledge. Previous RL approaches could handle high-dimensional feature space using a standard neural network, e.g., a convolutional neural network; however, to control traffic on a road network with multiple intersections, the geometric features between roads had to be created manually. Rather than using manually crafted geometric features, we developed an RL-based traffic signal control method that employs a graph convolutional neural network (GCNN). GCNNs can automatically extract features considering the traffic features between distant roads by stacking multiple neural network layers. We numerically evaluated the proposed method in a six-intersection environment. The results demonstrate that the proposed method can find comparable policies twice as fast as the conventional RL method with a neural network and can adapt to more extensive traffic demand changes.},
	urldate = {2024-07-01},
	booktitle = {2018 21st {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Nishi, Tomoki and Otaki, Keisuke and Hayakawa, Keiichiro and Yoshimura, Takayoshi},
	month = nov,
	year = {2018},
	note = {ISSN: 2153-0017},
	keywords = {Adaptation models, Convolutional neural networks, Feature extraction, multi-intersection, Roads, Stacking, Trajectory},
	pages = {877--883},
	file = {IEEE Xplore Abstract Record:/Users/mark.cortejo/Zotero/storage/WY56ZDUH/8569301.html:text/html;IEEE Xplore Full Text PDF:/Users/mark.cortejo/Zotero/storage/LFEU7A6A/Nishi et al. - 2018 - Traffic Signal Control Based on Reinforcement Lear.pdf:application/pdf},
}


@article{lansdowne_multi-agent_2006,
	title = {Multi-{Agent} {Traffic} {Simulation}},
	abstract = {This report follows the design and implementation of a traffic simulator written in NetLogo, an agent-based modelling environment. The system is developed with a view to analysing the likely effect of congestion-reducing schemes, in particular high occupancy vehicle lanes. The simulator is calibrated and validated using data from traffic studies, and then a section of road in South Gloucestershire is modelled to investigate the introduction of high occupancy vehicle lanes. The results suggest that the high occupancy vehicle lane fulfils its objectives by significantly reducing travel times for car sharers without adversely affecting other drivers.},
	language = {en},
        year = {2006},
	author = {Lansdowne, Andy},
	file = {Lansdowne - Multi-Agent Traffic Simulation.pdf:/Users/mark.cortejo/Zotero/storage/KZ8TJBE6/Lansdowne - Multi-Agent Traffic Simulation.pdf:application/pdf},
}


@article{newman_complex_2011,
	title = {Complex {Systems}: {A} {Survey}},
	volume = {79},
	issn = {0002-9505, 1943-2909},
	shorttitle = {Complex {Systems}},
	url = {http://arxiv.org/abs/1112.1440},
	doi = {10.1119/1.3590372},
	abstract = {A complex system is a system composed of many interacting parts, often called agents, which displays collective behavior that does not follow trivially from the behaviors of the individual parts. Examples include condensed matter systems, ecosystems, stock markets and economies, biological evolution, and indeed the whole of human society. Substantial progress has been made in the quantitative understanding of complex systems, particularly since the 1980s, using a combination of basic theory, much of it derived from physics, and computer simulation. The subject is a broad one, drawing on techniques and ideas from a wide range of areas. Here I give a survey of the main themes and methods of complex systems science and an annotated bibliography of resources, ranging from classic papers to recent books and reviews.},
	number = {8},
	urldate = {2024-03-12},
	journal = {American Journal of Physics},
	author = {Newman, M. E. J.},
	month = aug,
	year = {2011},
	note = {arXiv:1112.1440 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Physics and Society},
	pages = {800--810},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jet\\Zotero\\storage\\VC7Z3FY7\\Newman - 2011 - Complex Systems A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jet\\Zotero\\storage\\UUKA6U9Y\\1112.html:text/html},
}


@incollection{Lindsey2002, author = {Lindsey, R. and Verhoef, E.}, title = {Congestion Modelling}, booktitle = {Handbook of Transport Modelling}, edition = {1st reprint 2002}, publisher = {Pergamon}, pages = {377-397}, year = {2002} }